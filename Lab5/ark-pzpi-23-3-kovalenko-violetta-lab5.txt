Міністерство освіти та науки України


Харківський національний університет радіоелектроніки


Кафедра ПІ


ЗВІТ
з лабораторної роботи № 5
з дисципліни “Аналіз та рефакторинг коду
на тему:
“Розгортання програмної системи та демонстрація її роботи”








Виконала:                                                        
ст. гр.  ПЗПІ-23-3                                                       Коваленко В. О.


Перевірив:
ст. викладач кафедри ПІ                                            Сокорчук Ігор Петрович












Харків 2025
________________
1 ІСТОРІЯ ЗМІН
Таблиця 1.1 – Історія змін у звіті
№
	Дата
	Версія звіту
	Опис змін та виправлень
	1
	23.12.2025
	0.1
	Створено розділ “Завдання”
	2
	24.12.2025
	0.2
	Додано розділ “Опис виконаної роботи”
	3
	25.12.2025
	1.0
	Додано додатки А, Б, В
	________________
2 ЗАВДАННЯ
1. Розгорнути створену програмну систему:
   * Використовуючи інструменти для розгортання (наприклад, Docker, Kubernetes, хмарні сервіси), розгорнути серверну частину, веб-клієнт, мобільний клієнт та IoT клієнт.
   * Налаштувати середовище для роботи програмної системи (сервер, база даних, мережеві налаштування).
2. Перевірити та налаштувати роботу розгорнутої програмної системи:
   * Провести перевірку стабільності та коректності роботи усіх компонентів програмної системи (серверна частина, IoT клієнт, база даних, API).
   * Виконати налаштування параметрів серверної частини, клієнтських застосунків та мережевих протоколів для коректної роботи системи.
3. Продемонструвати описану у розділі 2.2 Vision & Scope функціональність програмної системи:
   * Описати та продемонструвати всі функції, передбачені в розділі 2.2 Vision & Scope, включаючи бізнес логіку, функції адміністрування та взаємодію між серверною частиною та клієнтами.
   * Перевірити реалізацію всіх необхідних функціональних можливостей програмної системи.
________________


3 ОПИС ВИКОНАНОЇ РОБОТИ
3.1 Вступ
3.2 Загальний опис системи
У межах виконання п’ятої лабораторної роботи мною було розгорнуто серверну частину, веб-клієнта та IoT клієнта. Дана система призначена для моніторингу фізичної активності користувачів, збору телеметричних даних, їх передачі на серверну частину та подальшого аналізу. IoT клієнт реалізує імітацію роботи фітнес-трекера, функціонально наближеного до реальних пристроїв типу Apple Watch, та забезпечує повноцінну взаємодію із сервером за допомогою REST API.
У процесі роботи система виконує генерацію даних від віртуальних датчиків, зокрема пульсу, кількості кроків та показників сну, після чого ці дані автоматично передаються на сервер для збереження у базі даних та подальшого використання в бізнес-логіці. Окрім цього, система забезпечує отримання персоналізованих планів дієти, рекомендацій та статистики активності.
3.3 Архітектурні рішення та використані технології
Система складається з трьох основних логічних компонентів, кожен з яких працює в окремому Docker-контейнері. Першим компонентом є IoT клієнт, реалізований мовою програмування Python. Він відповідає за імітацію роботи фітнес-трекера, генерацію телеметричних даних, локальну обробку статистики та взаємодію з користувачем через зручний інтерфейс. Другим компонентом є серверна частина, реалізована на платформі .NET Core, яка інкапсулює бізнес-логіку системи, забезпечує REST API та виконує обробку запитів від клієнтів. Третім компонентом є реляційна база даних Microsoft SQL Server, призначена для надійного збереження всіх даних системи.
Взаємодія між IoT клієнтом та серверною частиною реалізована через HTTP REST API. IoT клієнт звертається до серверного API за допомогою внутрішніх HTTP-клієнтів, використовуючи JSON як основний формат обміну даними. Сервер, у свою чергу, обробляє ці запити, виконує бізнес-логіку, взаємодіє з базою даних та повертає структуровані відповіді клієнту. Такий підхід дозволяє чітко розмежувати відповідальність між клієнтською та серверною частинами та забезпечує можливість незалежного масштабування кожного компонента.
Серверна частина системи побудована з використанням багатошарової архітектури. На рівні Presentation Layer реалізовано REST API за допомогою ASP.NET Core Web API. Саме цей рівень відповідає за прийом HTTP-запитів від IoT клієнта, маршрутизацію, первинну валідацію вхідних даних, автентифікацію та авторизацію. Для документування API використовується Swagger, що дозволяє візуально перевіряти роботу ендпоїнтів та спрощує інтеграцію з клієнтською частиною. Додатково на цьому рівні застосовуються middleware-компоненти для підтримки CORS, локалізації та базової автентифікації для адміністративних функцій.
Рівень бізнес-логіки реалізований у вигляді окремих сервісів, написаних мовою C#. У цьому шарі зосереджена вся ключова логіка системи, зокрема обробка телеметрії від IoT пристроїв, генерація денних планів дієти, формування рекомендацій, агрегація статистики активності та обробка профілів користувачів. Всі сервіси підключаються через механізм Dependency Injection, що дозволяє зменшити зв’язність між компонентами, спростити тестування та забезпечити гнучкість архітектури. Для передачі даних між шарами використовуються DTO-об’єкти з автоматичним маппінгом через AutoMapper, що дозволяє ізолювати внутрішні сутності бази даних від зовнішнього API.
Доступ до бази даних реалізовано на рівні Data Access Layer з використанням Entity Framework Core за підходом Code First. Для кожної ключової сутності системи створено окремий репозиторій, що інкапсулює логіку доступу до даних та забезпечує абстракцію від конкретної реалізації СУБД. DbContext відповідає за конфігурацію зв’язків між сутностями, створення таблиць, індексів та обмежень цілісності. Управління схемою бази даних здійснюється через механізм міграцій
База даних системи побудована на Microsoft SQL Server 2022 Express Edition та має реляційну модель. У ній зберігаються дані користувачів, профілі, зареєстровані пристрої, телеметричні зразки, записи сну, тренувань, плани дієти, рецепти та рекомендації. Для забезпечення цілісності даних використовуються первинні та зовнішні ключі, а також індекси для оптимізації запитів, що особливо важливо при роботі з великими обсягами телеметричних даних.
IoT клієнт реалізований як окремий GUI застосунок і виконує роль інтерактивного інтерфейсу між користувачем та серверною частиною. Він забезпечує запуск симуляції датчиків у фонових потоках, регулярну відправку телеметрії на сервер, локальне накопичення та обчислення статистики, а також відображення даних через браузер. Для стабільної роботи клієнта реалізовано механізми повторних HTTP-запитів, обробку мережевих помилок та конфігурацію таймаутів.
Важливим архітектурним рішенням є використання Docker та Docker Compose для розгортання всієї системи. Кожен компонент працює у власному контейнері, а взаємодія між ними здійснюється через внутрішню Docker-мережу. Це дозволяє розгорнути систему одним файлом docker-compose.yml, автоматично ініціалізувати базу даних, налаштувати залежності між сервісами та забезпечити стабільну роботу в різних середовищах. 
3.4 Розгортання системи
Для досягнення коректного розгортання системи я використала технологію контейнеризації Docker у поєднанні з Docker Compose. Це дозволило ізолювати кожен компонент системи в окремому контейнері, забезпечило взаємодію між ними 
3.4.1 Вимоги до системи
Перед початком розгортання я визначила мінімальні вимоги до апаратного та програмного забезпечення, необхідні для коректної роботи системи. Система може бути розгорнута на операційних системах Windows 10/11, Linux або macOS. Для коректної роботи Docker на Windows обов’язковою умовою є використання підсистеми WSL 2, оскільки саме вона забезпечує підтримку Linux-контейнерів.
Наявність Docker Desktop є ключовою вимогою, оскільки він включає в себе Docker Engine та Docker Compose, які використовуються для збірки образів і запуску контейнерів. 
3.4.2 Встановлення Docker та підготовка середовища
Перед безпосереднім запуском системи я перевіряю готовність середовища. На операційній системі Windows першочергово перевіряється наявність та версія WSL 2 за допомогою команди:
wsl --version
У випадку, якщо WSL не встановлений або використовується застаріла версія, я виконую його оновлення та встановлюю WSL 2 як версію за замовчуванням:


wsl --update
wsl --set-default-version 2


Після цього виконується встановлення Docker Desktop, яке здійснюється стандартним способом через офіційний інсталятор. Після завершення встановлення та перезавантаження системи я перевіряю доступність Docker та Docker Compose:


docker --version
docker-compose --version
3.4.3 Структура файлів розгортання
У корені проєкту розміщується файл docker-compose.yml, який відповідає за запуск усіх сервісів системи. Окремо розташовані директорії серверного застосунку та IoT-клієнта, кожна з яких містить власний Dockerfile.
3.4.4 docker-compose.yml як центральний елемент розгортання
Файл docker-compose.yml є ключовим компонентом усього процесу розгортання, оскільки саме в ньому описується повна інфраструктура системи, взаємодія між сервісами, змінні середовища, мережі та правила збереження даних.
У першому блоці я описую сервіс бази даних SQL Server. Використовується офіційний образ Microsoft SQL Server 2022. Пароль адміністратора передається через змінні середовища, що дозволяє уникнути жорсткого кодування конфіденційних даних у коді. Дані бази зберігаються у volume sqlserver_data
Healthcheck реалізований через виконання простого SQL-запиту, який дозволяє Docker визначити, чи готовий сервер бази даних до роботи. Програмний код наведено нижче:


services:
  sqlserver:
    image: mcr.microsoft.com/mssql/server:2022-latest
    container_name: fitness_sqlserver
    environment:
      - ACCEPT_EULA=Y
      - SA_PASSWORD=YourStrong@Passw0rd
      - MSSQL_PID=Express
    ports:
      - "1433:1433"
    volumes:
      - sqlserver_data:/var/opt/mssql
    healthcheck:
      test: ["CMD-SHELL", "/opt/mssql-tools18/bin/sqlcmd -S localhost -U sa -P 'YourStrong@Passw0rd' -C -Q 'SELECT 1' || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 30
      start_period: 60s
    restart: unless-stopped
    networks:
      - fitness_network


Наступним описується серверна частина API:


api:
  build:
    context: .
    dockerfile: FitnessProject/Dockerfile
  container_name: fitness_api
  ports:
    - "5006:5006"
    - "5007:5007"
  environment:
    - ASPNETCORE_ENVIRONMENT=Development
    - ASPNETCORE_URLS=http://+:5006
    - ConnectionStrings__DefaultConnection=Server=sqlserver;Database=FitnessProjectDB;User Id=sa;Password=YourStrong@Passw0rd;TrustServerCertificate=True;
  depends_on:
    - sqlserver
  restart: unless-stopped
  networks:
    - fitness_network


Тут я використовую власний Dockerfile для побудови ASP.NET Core API. Важливим моментом є використання імені сервісу sqlserver у рядку підключення до бази даних. Це обумовлено тим, що всередині Docker-мережі контейнери не бачать localhost один одного, а звертаються за DNS-іменами сервісів.


Останнім компонентом є IoT-клієнт:


iot_client:
  build:
    context: ./IoTClientFitnessProject
    dockerfile: Dockerfile
  container_name: fitness_iot_client
  environment:
    - SERVER_URL=http://api:5006
    - PYTHONUNBUFFERED=1
  volumes:
    - ./IoTClientFitnessProject/config:/app/config
  depends_on:
    - api
  ports:
    - "5000:5000"
  restart: unless-stopped
  networks:
    - fitness_network


IoT-клієнт отримує адресу серверного API через змінну середовища, що дозволяє змінювати конфігурацію без модифікації коду. Маппінг директорії config дозволяє редагувати налаштування клієнта без необхідності перебудови Docker-образу.
3.4.5 Dockerfile серверної частини
Для серверної частини я реалізуваkf multi-stage Dockerfile, що дозволяє відокремити процес збірки від фінального запуску застосунку.


FROM mcr.microsoft.com/dotnet/aspnet:8.0 AS base
WORKDIR /app
EXPOSE 5006
EXPOSE 5007


FROM mcr.microsoft.com/dotnet/sdk:8.0 AS build
WORKDIR /src
COPY ["FitnessProject/FitnessProject.csproj", "FitnessProject/"]
RUN dotnet restore "FitnessProject/FitnessProject.csproj"
COPY . .
WORKDIR "/src/FitnessProject"
RUN dotnet build "FitnessProject.csproj" -c Release -o /app/build


FROM build AS publish
RUN dotnet publish "FitnessProject.csproj" -c Release -o /app/publish


FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT ["dotnet", "FitnessProject.dll"]


3.4.6 Dockerfile IoT-клієнта
IoT-клієнт реалізований у вигляді Python-застосунку, який також контейнеризований:


FROM python:3.11-slim
WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt
COPY src/ ./src/
COPY config/ ./config/
ENV PYTHONUNBUFFERED=1
WORKDIR /app/src
EXPOSE 5000
CMD ["python", "web_app.py"]


3.4.7 Запуск та перевірка системи
Після підготовки всіх файлів я запускаю систему командою:


docker-compose up --build -d


У процесі запуску Docker автоматично збирає образи, створює контейнери, ініціалізує базу даних, застосовує міграції та запускає IoT-клієнт. Після завершення запуску система стає доступною для взаємодії через Swagger та веб-інтерфейс IoT-клієнта.
3.5. Обґрунтування архітектурних рішень та вибору технологій
3.5.1. Зміна архітектури IoT-клієнта
У процесі розробки IoT-клієнта для системи фітнес-трекінгу архітектура застосунку змінилась від простого настільного рішення до повноцінного веб-інтерфейсу, оптимізованого для роботи в Docker-середовищі. Кожен етап розвитку був обумовлений практичними обмеженнями середовища виконання та вимогами до зручності розгортання.
3.5.2 Початкова концепція: графічний інтерфейс на Tkinter
На початковому етапі я реалізувала IoT-клієнт у вигляді настільного застосунку з графічним інтерфейсом, використовуючи бібліотеку Tkinter, яка входить до стандартного набору Python. Таке рішення було логічним, оскільки дозволяло швидко створити інтерфейс для локального тестування без підключення додаткових технологій.
Графічний інтерфейс будувався навколо центрального класу GUI, який відповідав за ініціалізацію вікна, вкладок та елементів керування. Точка входу запускала цей інтерфейс у стандартному режимі виконання Python-застосунку. У файлі watch_gui.py я створювала головне вікно застосунку, вкладки та елементи інтерфейсу, використовуючи стандартні компоненти Tkinter.


# watch_gui.py
import tkinter as tk
from tkinter import ttk


class WatchGUI:
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("IoT Fitness Watch")


        self.notebook = ttk.Notebook(self.root)
        self.notebook.pack(expand=True, fill="both")


        self.create_tabs()


    def create_tabs(self):
        self.main_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.main_tab, text="Головна")


        self.stats_tab = ttk.Frame(self.notebook)
        self.notebook.add(self.stats_tab, text="Статистика")


    def run(self):
        self.root.mainloop()


Такий підхід дозволяв працювати з бізнес-логікою клієнта, отримувати дані з API, відображати статистику та керувати симуляцією датчиків у зручному графічному середовищі.
3.5.3 Проблема запуску GUI у Docker-середовищі
Після переходу до контейнеризації всієї системи я зіткнулася з проблемою: Docker-контейнери за замовчуванням не мають графічного середовища. Tkinter, як і будь-який GUI-фреймворк для настільних застосунків, потребує X11-сервер для відображення вікон, якого у контейнері немає.
Щоб обійти це обмеження, я реалізувала варіант запуску графічного інтерфейсу через віртуальний дисплей Xvfb у поєднанні з VNC-сервером. Для цього в Docker-образ було додано відповідні пакети, а запуск GUI відбувався у віртуальному X-середовищі.


RUN apt-get update && apt-get install -y \
    python3-tk \
    xvfb \
    x11vnc \
    fluxbox \
    websockify


Запуск сервісів відбувався через entrypoint-скрипт, де я вручну ініціалізувала віртуальний дисплей та VNC-доступ.


Xvfb :99 -screen 0 1024x768x24 &
export DISPLAY=:99


x11vnc -display :99 -listen 0.0.0.0 -rfbport 5900 &
websockify 6080 localhost:5900 &


На практиці такий підхід виявився надмірно складним і нестабільним. Під час тестування виникали проблеми з некоректним відображенням вікон, інверсією кольорів, втратою фокусу та затримками у взаємодії. Крім того, VNC передавав повне зображення екрану, що суттєво збільшувало навантаження на систему та мережу. У підсумку стало зрозуміло, що використання настільного GUI у контейнеризованому середовищі є архітектурно невдалим рішенням.
3.5.4 Перехід на веб-інтерфейс з використанням Flask
Проаналізувавши всі проблеми, я прийняла рішення повністю відмовитися від настільного GUI та перейти на веб-інтерфейс, реалізований за допомогою Flask. Це дозволило адаптувати IoT-клієнт до Docker-середовища без використання будь-яких графічних серверів.
Flask був обраний як легковаговий веб-фреймворк, який легко інтегрується з існуючим кодом і ідеально підходить для ролі тонкої обгортки над бізнес-логікою.


# web_app.py
from flask import Flask, jsonify, render_template
from api_client import ApiClient


app = Flask(__name__)
api_client = ApiClient()


@app.route("/")
def index():
    return render_template("index.html")


@app.route("/api/statistics/day")
def day_statistics():
    stats = api_client.get_daily_statistics()
    return jsonify(stats or {})


У такій архітектурі Flask не містить логіки обробки даних, а лише приймає HTTP-запити від браузера, викликає відповідні модулі та повертає результат у форматі JSON.
3.5.5 Архітектура веб-інтерфейсу на Flask
Веб-інтерфейс побудований за класичною схемою поділу відповідальності між серверною та клієнтською частинами. HTML-шаблон містить усі вкладки, які раніше існували в Tkinter-інтерфейсі, а JavaScript виконує асинхронні запити до API. Таким чином, веб-інтерфейс повністю замінив настільний GUI, зберігши всю функціональність і водночас значно спростивши розгортання системи.
3.5.6 Висновки щодо архітектурних рішень
У результаті переходу від Tkinter-інтерфейсу з VNC до Flask-веб-інтерфейсу я отримала значно простішу, стабільнішу та масштабованішу архітектуру. Веб-підхід повністю відповідає вимогам контейнеризації, зменшує кількість залежностей і забезпечує зручний доступ до системи з будь-якого пристрою.
3.6 Допоміжні скрипти для управління системою
У процесі розгортання IoT-системи фітнес-трекінгу я зіткнулася з необхідністю спростити керування Docker-контейнерами. Оскільки система складається з кількох сервісів і запускається через Docker Compose, ручне введення команд у консолі є незручним. Саме тому було прийнято рішення створити допоміжні batch-скрипти для операційної системи Windows, які автоматизують запуск та зупинку всієї системи. Ці скрипти виконують роль проміжного шару між користувачем і Docker, приховуючи складність команд та забезпечуючи зрозумілий і безпечний сценарій запуску.
3.6.1. Загальна ідея та призначення скриптів
Створені скрипти дозволяють керувати всією системою за допомогою одного однієї команди та виводять допоміжну, але доволі важливу інформацію на консоль. Вони перевіряють наявність необхідних інструментів, інформують користувача про поточний стан системи та виконують стандартні Docker Compose операції у правильній послідовності. Для цього були реалізовані два основні файли: docker-start.bat, який відповідає за запуск системи, та docker-stop.bat, який коректно зупиняє всі контейнери.
3.6.2 Скрипт docker-start.bat – запуск системи
Скрипт docker-start.bat використовується для повного запуску всієї IoT-системи у Docker-середовищі. Його головне завдання – перевірити готовність системи та автоматично виконати команду docker-compose up із необхідними параметрами. Повний вміст файлу має такий вигляд:


@echo off
chcp 65001 >nul
echo ========================================
echo Docker Setup - Fitness Project
echo ========================================
echo.


echo Перевірка Docker...
docker --version >nul 2>&1
if %errorlevel% neq 0 (
    echo ПОМИЛКА: Docker не встановлено або не запущено!
    echo.
    echo Встановіть Docker Desktop з: https://www.docker.com/products/docker-desktop/
    echo Переконайтесь, що Docker Desktop запущений.
    pause
    exit /b 1
)


echo Docker знайдено!
echo.


echo Перевірка Docker Compose...
docker-compose --version >nul 2>&1
if %errorlevel% neq 0 (
    echo ПОМИЛКА: Docker Compose не знайдено!
    pause
    exit /b 1
)


echo Docker Compose знайдено!
echo.


echo ========================================
echo Запуск системи...
echo ========================================
echo.


docker-compose up --build -d


echo.
echo ========================================
echo Система запущена!
echo.
echo Swagger API: http://localhost:5006/swagger
echo IoT Client (Web): http://localhost:5000
echo ========================================
echo.


pause


На самому початку скрипта я вимикаю стандартний вивід команд за допомогою @echo off, щоб консоль не була перевантажена технічними повідомленнями. Одразу після цього виконується команда chcp 65001, яка встановлює кодування UTF-8. Це необхідно для коректного відображення українського тексту в командному рядку Windows.
Далі скрипт перевіряє, чи встановлений і запущений Docker. Для цього використовується команда docker --version. Якщо Docker відсутній або недоступний, змінна errorlevel буде ненульовою, і скрипт виведе зрозуміле повідомлення з поясненням проблеми та посиланням на офіційний сайт Docker Desktop. У такому випадку виконання сценарію завершується, що запобігає подальшим помилкам.
Аналогічним чином виконується перевірка наявності Docker Compose, без якого неможливо оркеструвати запуск кількох контейнерів одночасно. Лише після успішного проходження всіх перевірок скрипт переходить до основної частини запуску системи.
Команда


docker-compose up --build -d


виконує одразу кілька важливих дій. Параметр up запускає всі сервіси, описані у файлі docker-compose.yml. Ключ --build примусово перебудовує Docker-образи, що гарантує використання актуальної версії коду. Параметр -d запускає контейнери у фоновому режимі, дозволяючи користувачу одразу продовжити роботу з консоллю. Після завершення запуску скрипт виводить адресу веб-інтерфейсу IoT-клієнта та Swagger-документації серверного API, що значно спрощує доступ до системи.
3.6.3 Скрипт docker-stop.bat – зупинка системи
Для коректного завершення роботи системи було створено окремий скрипт docker-stop.bat. Його призначення – безпечно зупинити всі контейнери та звільнити ресурси.


@echo off
chcp 65001 >nul
echo ========================================
echo Зупинка Docker контейнерів
echo ========================================
echo.


docker-compose down


echo.
echo Контейнери зупинено!
echo.


pause


Цей скрипт використовує команду docker-compose down, яка зупиняє всі сервіси, видаляє контейнери та мережу, створену Docker Compose. При цьому volumes не видаляються, що дозволяє зберегти всі дані, зокрема дані бази даних. 
3.6.4 Практичне використання скриптів
Скрипти призначені для максимально простого використання. Користувач може запустити або зупинити систему шляхом виклику його з командного рядка. Усі необхідні дії виконуються автоматично, без потреби вручну вводити Docker-команди чи перевіряти стан сервісів.
________________


ВИСНОВОК
У ході виконання лабораторної роботи було успішно здійснено розгортання серверної частини програмної системи з використанням технології контейнеризації Docker. Я реалізувала повноцінне ізольоване середовище виконання, яке дозволяє запускати систему незалежно від конфігурації хостової операційної системи.
Для розгортання системи було створено Docker-образи для всіх основних компонентів серверної інфраструктури. У Dockerfile я визначила необхідне середовище виконання, встановлення залежностей, копіювання вихідного коду та команду запуску серверного застосунку. 
З метою організації взаємодії між компонентами системи було використано Docker Compose. Я налаштувала конфігурацію сервісів, мереж та змінних середовища, що дозволило коректно зв’язати серверний застосунок з базою даних та іншими допоміжними сервісами. 
Окрему увагу було приділено роботі з мережами Docker. Я налаштувала внутрішню мережу між контейнерами, що забезпечує стабільний обмін даними між сервісами без необхідності відкривати зайві порти назовні. 
У процесі розгортання також було реалізовано збереження даних за допомогою Docker volumes. Це дозволило забезпечити збереження даних бази даних між перезапусками контейнерів та уникнути їх втрати при оновленні або повторному розгортанні системи.
Таким чином, використання Docker дозволило реалізувати сучасний, масштабований та зручний механізм розгортання серверної частини програмної системи. 
________________
ДОДАТОК А
Відеозапис
Відеозапис доповіді: https://youtu.be/MLz_t326V5o
Хронологічний опис відеозапису:
00:00 - Вступ
00:10 - Загальний опис системи
00:45 - Архітектурні рішення та використані технології
01:12 - Розгортання системи
01:25 - Вимоги до системи
01:42 - docker-compose.yml як центральний елемент розгортання
02:32 - Dockerfile серверної частини
02:43 - Dockerfile IoT клієнта
02:52 - Запуск та перевірка системи
03:08 - Обгрунтування архітектурних рішень та вибору технології
04:38 - Архітектура веб-інтерфейсу на Flask
04:55 - Допоміжні скрипти для управління системою
05:42 - Підсумок теоретичної частини
06:11 - Практична частина
06:25 - Розгортання системи
06:57 - Перегляд Desktop docker
07:21 - Перевірка роботи IoT клієнта
08:32 - Перевірка роботи серверної частини
09:35 - Зупинка роботи Docker
10:05 - Дякую за увагу!